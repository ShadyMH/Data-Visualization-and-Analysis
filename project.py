# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jmWTxdVty7MjAKqqwkfDlQm-xRWgod65

# **Importing Datasets**
"""

import gdown
gdown.download('https://drive.google.com/uc?id=1bimJIVcVPMd1i4u6eGqs1FgdNtG-kbvC', 'data.xlsx', quiet=False)
gdown.download('https://drive.google.com/uc?id=1BasIkcXt4aK5En-5ZlopojXGQiWWNZH7', 'data2.xlsx', quiet=False)
gdown.download('https://drive.google.com/uc?id=1LxKiSEc8pQ8qPXdmIdTl3TbCycW5X1XD', 'data3.xlsx', quiet=False)
gdown.download('https://drive.google.com/uc?id=1mdklACzF8Vey-KIEMVpWHw_EB7YPDl9m', 'data4.xlsx', quiet=False)
gdown.download('https://drive.google.com/uc?id=14xSUV_qZzPqacybJig3BpFWBgli_1Np8', 'data5.xlsx', quiet=False)

""" # **Reading and fixing the dataframes**"""

import pandas as pd
import numpy as np
day_names_hebrew = {'Sunday': 'ראשון', 'Monday': 'שני', 'Tuesday': 'שלישי', 'Wednesday': 'רביעי', 'Thursday': 'חמישי', 'Friday': 'שישי', 'Saturday': 'שבת'}

# Read the data sets into a DataFrame
df1 = pd.read_excel('data.xlsx')
df2 = pd.read_excel('data2.xlsx', header=3, sheet_name=1)
df3 = pd.read_excel('data3.xlsx')
df4 = pd.read_excel('data4.xlsx')
df5 = pd.read_excel('data5.xlsx')

# Change date format
df3['זמן_סיום_מהמערכת'] = pd.to_datetime(df3['זמן_סיום_מהמערכת'], format='%d-%m-%Y  %H:%M:%S').dt.strftime('%Y-%m-%d')
df1['זמן_סיום_מהמערכת'] = pd.to_datetime(df1['זמן_סיום_מהמערכת'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')
df4['זמן_סיום_מהמערכת'] = pd.to_datetime(df4['זמן_סיום_מהמערכת'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')
df5['זמן_סיום_מהמערכת'] = pd.to_datetime(df5['זמן_סיום_מהמערכת'], format='%d-%m-%Y  %H:%M:%S').dt.strftime('%Y-%m-%d')

#Remove duplicated rows (only max value)
df3_sorted = df3.sort_values(by='מפלס הפסולת בפח במספר', ascending=False)
df3 = df3_sorted.drop_duplicates(subset=['זמן_סיום_מהמערכת', 'time', 'כתובת_תיאור_מיקום_נקודת_המדידה'], keep='first')

# Replace values
df1['gender'] = df1['gender'].replace({'זכר': 1, 'נקבה': 0})
df1['תיאור_חיובי_שלילי'] = df1['תיאור_חיובי_שלילי'].replace({'חיובית': 1, 'שלילית': 0})
df1['age'] = df1['age'].replace({'70 ומעלה': '70-90'})
df4['מדרכה'] = df4['מדרכה'].replace({'לא רלוונטי': 0, 'סביר': 1, 'לא תקין (מוזנח)': 2, 'תקין (מטופח)': 3})
df4['אבנישפה'] = df4['אבנישפה'].replace({'לא רלוונטי': 0, 'סביר': 1, 'לא תקין (מוזנח)': 2, 'תקין (מטופח)': 3})
df4['גדרות'] = df4['גדרות'].replace({'לא רלוונטי': 0, 'סביר': 1, 'לא תקין (מוזנח)': 2, 'תקין (מטופח)': 3})
df4['צמחייה'] = df4['צמחייה'].replace({'לא רלוונטי': 0, 'סביר': 1, 'לא תקין (מוזנח)': 2, 'תקין (מטופח)': 3})

#Fix data (drop nan rows)
df3 = df3.dropna(subset=['time', 'נ.צ כתובת', 'זמן_סיום_מהמערכת'])
df4 = df4.dropna(subset=['time', 'נ.צ כתובת', 'זמן_סיום_מהמערכת'])
df5 = df5.dropna(subset=['time', 'נ.צ כתובת', 'זמן_סיום_מהמערכת'])

""" # **Merging**"""

#Hour column for merging
df1.loc[:, 'hour'] = pd.to_datetime(df1['time'].astype(str)).dt.hour
df3.loc[:, 'hour'] = pd.to_datetime(df3['time'].astype(str)).dt.hour
df4.loc[:, 'hour'] = pd.to_datetime(df4['time'].astype(str)).dt.hour
df5.loc[:, 'hour'] = pd.to_datetime(df5['time'].astype(str)).dt.hour

# Merge more data
temp_1 = pd.merge(df1, df2[['סכום  אוכלוסייה בסוף השנה', 'שטח', 'דירוג חברתי-כלכלי ','מדד סוציואקונומי', 'סמל_יישוב']], on='סמל_יישוב', how='left')
temp_2 = pd.merge(temp_1, df4[['מדרכה', 'אבנישפה', 'גדרות', 'צמחייה', 'נ.צ כתובת', 'זמן_סיום_מהמערכת', 'hour']], on=['נ.צ כתובת', 'זמן_סיום_מהמערכת', 'hour'], how='left')
#temp_3 = pd.merge(temp_2, df5[['אריזות קרטון','כתמי מסטיק' ,'צואת כלבים' ,'פסולת אורגנית','פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות' ,'זכוכית לא מכלי משקה או לא ניתן לזיהוי' ,'אריזות של חטיפים' ,'כוסות שתייה חמה חדפ','כוסות שתייה קרה חדפ','סכום חדפ','צלחות חדפ','אריזות מזון Take Away נייר','פקקים של מכלי משקה','מכלי משקה למיניהם','קופסאות סיגריות','מסכות כירורגיות','בדלי סיגריות','סוג_נקודת_המדידה_תשובה', 'זמן_סיום_מהמערכת','נ.צ כתובת']], on=['סוג_נקודת_המדידה_תשובה', 'זמן_סיום_מהמערכת','נ.צ כתובת'], how='left')
temp_3 = pd.merge(temp_2, df5[['אריזות קרטון','כתמי מסטיק' ,'צואת כלבים' ,'פסולת אורגנית','פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות' ,'זכוכית לא מכלי משקה או לא ניתן לזיהוי' ,'אריזות של חטיפים' ,'כוסות שתייה חמה חדפ','כוסות שתייה קרה חדפ','סכום חדפ','צלחות חדפ','אריזות מזון Take Away נייר','פקקים של מכלי משקה','מכלי משקה למיניהם','קופסאות סיגריות','מסכות כירורגיות','בדלי סיגריות','hour', 'זמן_סיום_מהמערכת','נ.צ כתובת']], on=['hour', 'זמן_סיום_מהמערכת','נ.צ כתובת'], how='left')
merged_df = pd.merge(temp_3, df3[['מצב הפח','כמה פחים יש בנקודת המדידה', 'מפלס הפסולת בפח במספר', 'נ.צ כתובת', 'זמן_סיום_מהמערכת', 'hour']], on=['נ.צ כתובת', 'זמן_סיום_מהמערכת', 'hour'], how='left')

""" # **Fixing Age and Time columns**"""

# Change the Age column to values
def get_avg(age_range):
    if isinstance(age_range, float):
        if np.isnan(age_range):
            return None
        else:
            return int(age_range)

    elif isinstance(age_range, str):
        if age_range not in ['', 'nan']:
            nums = [int(x) for x in age_range.split('-')]
            return int(np.mean(nums))
        else:
            return None

    else:
        return None

# Apply the function to the Age column
merged_df['age'] = merged_df['age'].astype(str)
merged_df['age'] = merged_df['age'].apply(get_avg)

# Round the Time column
merged_df['time'] = pd.to_datetime(merged_df['time'])
def round_time(t):
    hour = t.hour
    if t.minute >= 30:
        hour += 1
    return hour

merged_df['time'] = merged_df['time'].apply(round_time)

""" # **Seperating Date column and turning columns to Dummy vars**"""

# Seperate the Date column
merged_df['זמן_סיום_מהמערכת'] = pd.to_datetime(merged_df['זמן_סיום_מהמערכת'])
merged_df['יום'] = merged_df['זמן_סיום_מהמערכת'].dt.day
merged_df['חודש'] = merged_df['זמן_סיום_מהמערכת'].dt.month
merged_df['יום בשבוע'] = merged_df['זמן_סיום_מהמערכת'].dt.day_name().map(day_names_hebrew)

# Change תיאור column to הולך רגל and רכב dummy variables
merged_df['הולך רגל'] = 1
merged_df['רכב'] = merged_df['תיאור_טקסט'].str.contains('רכב|מכונית|נהג|משאית|אוטו|אופנוע|אופני|קורקינט|נסע', case=False, regex=True)
merged_df.loc[merged_df['רכב'] == 1, 'הולך רגל'] = 0
merged_df['רכב'] = merged_df['רכב'].fillna(0)
merged_df['רכב'] = merged_df['רכב'].astype(int)
merged_df['הולך רגל'] = merged_df['הולך רגל'].astype(int)

# Change to dummy variables
merged_df = merged_df.join(pd.get_dummies(merged_df['סוג_נקודת_המדידה_תשובה'], dtype=int))
merged_df = merged_df.join(pd.get_dummies(merged_df['יום בשבוע'], dtype=int))

#Change פריט column to dummy variables
merged_df['פריט אחר'] = 1

merged_df['כוס'] = merged_df['פריט'].str.contains('כוס|ספל', case=False, regex=True)
merged_df.loc[merged_df['כוס'] == 1, 'פריט אחר'] = 0
merged_df['כוס'] = merged_df['כוס'].fillna(0)
merged_df['כוס'] = merged_df['כוס'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['שקיות'] = merged_df['פריט'].str.contains('שקי', case=False, regex=True)
merged_df.loc[merged_df['שקיות'] == 1, 'פריט אחר'] = 0
merged_df['שקיות'] = merged_df['שקיות'].fillna(0)
merged_df['שקיות'] = merged_df['שקיות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['אריזות'] = merged_df['פריט'].str.contains('אריז', case=False, regex=True)
merged_df.loc[merged_df['אריזות'] == 1, 'פריט אחר'] = 0
merged_df['אריזות'] = merged_df['אריזות'].fillna(0)
merged_df['אריזות'] = merged_df['אריזות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['סיגריות'] = merged_df['פריט'].str.contains('בדל|סיגר|קופסאת סיגריות|סגריות', case=False, regex=True)
merged_df.loc[merged_df['סיגריות'] == 1, 'פריט אחר'] = 0
merged_df['סיגריות'] = merged_df['סיגריות'].fillna(0)
merged_df['סיגריות'] = merged_df['סיגריות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['בקבוקים'] = merged_df['פריט'].str.contains('בקבוק|פקק', case=False, regex=True)
merged_df.loc[merged_df['בקבוקים'] == 1, 'פריט אחר'] = 0
merged_df['בקבוקים'] = merged_df['בקבוקים'].fillna(0)
merged_df['בקבוקים'] = merged_df['בקבוקים'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['צרכים בע"ח'] = merged_df['פריט'].str.contains('קקי|צואה|כלב|צואת', case=False, regex=True)
merged_df.loc[merged_df['צרכים בע"ח'] == 1, 'פריט אחר'] = 0
merged_df['צרכים בע"ח'] = merged_df['צרכים בע"ח'].fillna(0)
merged_df['צרכים בע"ח'] = merged_df['צרכים בע"ח'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['פלסטיק'] = merged_df['פריט'].str.contains('צלחת|קש|חד"פ', case=False, regex=True)
merged_df.loc[merged_df['פלסטיק'] == 1, 'פריט אחר'] = 0
merged_df['פלסטיק'] = merged_df['פלסטיק'].fillna(0)
merged_df['פלסטיק'] = merged_df['פלסטיק'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['מסיכות'] = merged_df['פריט'].str.contains('מסיכה|מסכה|מסיכת|מסכת', case=False, regex=True)
merged_df.loc[merged_df['מסיכות'] == 1, 'פריט אחר'] = 0
merged_df['מסיכות'] = merged_df['מסיכות'].fillna(0)
merged_df['מסיכות'] = merged_df['מסיכות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['פחיות'] = merged_df['פריט'].str.contains('פחית', case=False, regex=True)
merged_df.loc[merged_df['פחיות'] == 1, 'פריט אחר'] = 0
merged_df['פחיות'] = merged_df['פחיות'].fillna(0)
merged_df['פחיות'] = merged_df['פחיות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['נייר'] = merged_df['פריט'].str.contains('נייר|קבלה|קבלת', case=False, regex=True)
merged_df.loc[merged_df['נייר'] == 1, 'פריט אחר'] = 0
merged_df['נייר'] = merged_df['נייר'].fillna(0)
merged_df['נייר'] = merged_df['נייר'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

merged_df['עטיפות'] = merged_df['פריט'].str.contains('עטיפה|עטיפת', case=False, regex=True)
merged_df.loc[merged_df['עטיפות'] == 1, 'פריט אחר'] = 0
merged_df['עטיפות'] = merged_df['עטיפות'].fillna(0)
merged_df['עטיפות'] = merged_df['עטיפות'].astype(int)
merged_df['פריט אחר'] = merged_df['פריט אחר'].astype(int)

# Adding the week before and after
merged_df['שבוע לפני'] = merged_df['מצב הפח'].shift(periods=7)
merged_df['שבוע אחרי'] = merged_df['מצב הפח'].shift(periods=-7)

#if the bin is torn 0 if broken 1
merged_df['מצב הפח'] = merged_df['מצב הפח'].replace({'בלוי': 0, 'שבור': 1})

"""# **Dropping unnecessary columns and exporting data_clean**"""

# Drop columns
merged_df = merged_df.drop(['point_type', 'סוג_נקודת_המדידה_תשובה', 'Index1', 'heged1', 'heged2', 'heged3', 'heged4', 'heged5', 'heged6', 'heged7', 'heged8', 'heged9', 'heged10', 'heged11', 'heged12', 'heged13', 'תיאור_טקסט', 'hour', 'יום בשבוע'], axis=1)

# Save the modified DataFrame back to a new Excel file
merged_df['זמן_סיום_מהמערכת'] = pd.to_datetime(merged_df['זמן_סיום_מהמערכת'], format='%Y-%m-%d %H:%M:%s').dt.strftime('%d-%m-%Y')
merged_df.to_excel('data_clean.xlsx', index=False)
print("data_clean has been created.")

"""# **Making DF only negative reports for stats**"""

df_only_neg = merged_df[merged_df['תיאור_חיובי_שלילי'] == 0]

df_only_neg.to_excel('df_only_neg.xlsx', index=False)
print("data_only_neg has been created.")

"""## **Statistical Models**"""

df_only_neg = df_only_neg.dropna(subset=['time'])
df_only_neg = df_only_neg.dropna(subset=['age'])

"""## **Chi-Square Test**

## **Gender**
"""

#CHI SQUARE ON GENDER
from scipy.stats import chi2_contingency

men_sum = (df_only_neg['gender'] == 1).sum()
women_sum = (df_only_neg['gender'] == 0).sum()
print("Number of false Men:", men_sum)
print("Number of false Women:", women_sum)
print("Number of rows in df_only_false:", df_only_neg.shape[0])

temp_gender = [[(0.5035 * df_only_neg.shape[0]), (0.4965 * df_only_neg.shape[0])],
               [women_sum, men_sum]]
df_gender_false = pd.DataFrame(temp_gender)
df_gender_false.columns = ['Female', 'Male']
df_gender_false.index = ['Expected', 'Observed']
print(df_gender_false)

#Perform the chi-square test on Gender
chi2_stat, p_val, dof, expected = chi2_contingency(df_gender_false)

print("\nChi-square statistic:", chi2_stat)
print("P-value:", p_val)
print("Degrees of freedom:", dof)
print("\nExpected frequencies:")
print(expected)

"""## **Age**"""

age_group_0_19 = df_only_neg[(df_only_neg['age'] >= 0) & (df_only_neg['age'] <= 19)]
age_group_20_29 = df_only_neg[(df_only_neg['age'] >= 20) & (df_only_neg['age'] <= 29)]
age_group_30_plus = df_only_neg[df_only_neg['age'] >= 30]

# Create contingency table
contingency_table = pd.DataFrame({'Observed': [len(age_group_0_19), len(age_group_20_29), len(age_group_30_plus)]})

total = contingency_table['Observed'].sum()
contingency_table['Expected'] = total / 3

print("Contingency Table:")
print(contingency_table)

# Perform chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency([contingency_table['Observed'], contingency_table['Expected']])

print("\nChi-square statistic:", chi2_stat)
print("P-value:", p_val)
print("Degrees of freedom:", dof)
print("\nExpected frequencies:")
print(expected)

"""## **Days of the week**"""

sunday = df_only_neg[df_only_neg['ראשון'] == 1]
monday = df_only_neg[df_only_neg['שני'] == 1]
tuesday = df_only_neg[df_only_neg['שלישי'] == 1]
wednesday = df_only_neg[df_only_neg['רביעי'] == 1]
thursday = df_only_neg[df_only_neg['חמישי'] == 1]
friday = df_only_neg[df_only_neg['שישי'] == 1]
saturday = df_only_neg[df_only_neg['שבת'] == 1]

# Create a dataframe to store the results
contingency_table = pd.DataFrame({'Observed': [len(sunday), len(monday),len(tuesday),len(wednesday),len(thursday),len(friday),len(saturday)]})

total = contingency_table['Observed'].sum()
contingency_table['Expected'] = total / 7

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform the chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency([contingency_table['Observed'], contingency_table['Expected']])

print("\nChi-square statistic:", chi2_stat)
print("P-value:", p_val)
print("Degrees of freedom:", dof)
print("\nExpected frequencies:")
print(expected)

"""## **On foot / Car**"""

onfoot = df_only_neg[df_only_neg['הולך רגל'] == 1]
car = df_only_neg[df_only_neg['רכב'] == 1]

# Create a dataframe to store the results
contingency_table = pd.DataFrame({'Observed': [len(onfoot), len(car)]})

total = contingency_table['Observed'].sum()
contingency_table['Expected'] = total / 2

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform the chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency([contingency_table['Observed'], contingency_table['Expected']])

print("\nChi-square statistic:", chi2_stat)
print("P-value:", p_val)
print("Degrees of freedom:", dof)
print("\nExpected frequencies:")
print(expected)

"""## **Different Areas**"""

indus_area = df_only_neg[df_only_neg['אזור תעשייה'] == 1]
beach = df_only_neg[df_only_neg['חוף ים'] == 1]
public_building = df_only_neg[df_only_neg['מבנה ציבור'] == 1]
com_center = df_only_neg[df_only_neg['מרכז מסחרי'] == 1]
fun = df_only_neg[df_only_neg['פנאי ונופש'] == 1]
res_street = df_only_neg[df_only_neg['רחוב מגורים'] == 1]
indus_street = df_only_neg[df_only_neg['רחוב מסחרי'] == 1]

# Create a dataframe to store the results
contingency_table = pd.DataFrame({'Observed': [len(indus_area), len(beach),len(public_building),len(com_center),len(fun),len(res_street),len(indus_street)]})

total = contingency_table['Observed'].sum()
contingency_table['Expected'] = total / 7

# Display the contingency table
print("Contingency Table:")
print(contingency_table)

# Perform the chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency([contingency_table['Observed'], contingency_table['Expected']])

print("\nChi-square statistic:", chi2_stat)
print("P-value:", p_val)
print("Degrees of freedom:", dof)
print("\nExpected frequencies:")
print(expected)

"""# **Correlation**

# **Area size**
"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr

new_tmp = pd.DataFrame(df_only_neg)
new_tmp = new_tmp[new_tmp['יישוב'] != 'חבל אילות']

# Convert the size column to numeric without producing the warning
new_tmp.loc[:, 'שטח'] = pd.to_numeric(new_tmp['שטח'], errors='coerce')

# Group by city size and count the number of occurrences
city_size_counts = new_tmp['שטח'].value_counts().reset_index()
city_size_counts.columns = ['שטח', 'מספר התלונות']

# Calculate the Pearson correlation coefficient
#correlation = city_size_counts['שטח'].corr(city_size_counts['Count'])
correlation, p_value = spearmanr(city_size_counts['שטח'], city_size_counts['מספר התלונות'])

print("Correlation coefficient:", correlation)
print("P-value:", p_value)

plt.figure(figsize=(10, 6))
sns.regplot(x='שטח', y='מספר התלונות', data=city_size_counts)
plt.title('City Size vs Waste Count')
plt.xlabel('City Area')
plt.ylabel('Waste Count')
plt.grid(True)
plt.text(0.1, 0.9, f'Correlation coefficient: {correlation:.2f}\nP-value: {p_value:.2f}',
         transform=plt.gca().transAxes)
plt.show()

"""# **Bins**"""

new_tmp_2 = pd.DataFrame(df_only_neg)
new_tmp_2.dropna(subset=['מפלס הפסולת בפח במספר'], inplace=True)
new_tmp_2.dropna(subset=['כמה פחים יש בנקודת המדידה'], inplace=True)

# Convert the size column to numeric without producing the warning
new_tmp_2.loc[:, 'כמה פחים יש בנקודת המדידה'] = pd.to_numeric(new_tmp_2['כמה פחים יש בנקודת המדידה'], errors='coerce')

# Group by city size and count the number of occurrences
city_size_counts = new_tmp_2['כמה פחים יש בנקודת המדידה'].value_counts().reset_index()
city_size_counts.columns = ['כמה פחים יש בנקודת המדידה', 'מספר התלונות']

# Calculate the Pearson correlation coefficient
#correlation = city_size_counts['שטח'].corr(city_size_counts['Count'])
correlation, p_value = spearmanr(city_size_counts['כמה פחים יש בנקודת המדידה'], city_size_counts['מספר התלונות'])

print("Correlation coefficient:", correlation)
print("P-value:", p_value)

plt.figure(figsize=(10, 6))
sns.regplot(x='כמה פחים יש בנקודת המדידה', y='מספר התלונות', data=city_size_counts)
plt.title('Number of Bins vs Waste Count')
plt.xlabel('Number of Bins')
plt.ylabel('Waste Count')
plt.grid(True)
plt.text(0.1, 0.9, f'Correlation coefficient: {correlation:.2f}\nP-value: {p_value:.2f}',
         transform=plt.gca().transAxes)
plt.show()

# Convert the size column to numeric without producing the warning
new_tmp_2.loc[:, 'מפלס הפסולת בפח במספר'] = pd.to_numeric(new_tmp_2['מפלס הפסולת בפח במספר'], errors='coerce')

# Group by city size and count the number of occurrences
city_size_counts = new_tmp_2['מפלס הפסולת בפח במספר'].value_counts().reset_index()
city_size_counts.columns = ['מפלס הפסולת בפח במספר', 'מספר התלונות']

# Calculate the Pearson correlation coefficient
#correlation = city_size_counts['שטח'].corr(city_size_counts['Count'])
correlation, p_value = spearmanr(city_size_counts['מפלס הפסולת בפח במספר'], city_size_counts['מספר התלונות'])

print("Correlation coefficient:", correlation)
print("P-value:", p_value)

plt.figure(figsize=(10, 6))
sns.regplot(x='מפלס הפסולת בפח במספר', y='מספר התלונות', data=city_size_counts)
plt.title('Waste level of Bins vs Waste Count')
plt.xlabel('Waste level of Bins')
plt.ylabel('Waste Count')
plt.grid(True)
plt.text(0.1, 0.9, f'Correlation coefficient: {correlation:.2f}\nP-value: {p_value:.2f}',
         transform=plt.gca().transAxes)
plt.show()

"""# **Changing DF for the model**"""

# Combine date, time, and address into a single ID column
df_only_neg['זמן_סיום_מהמערכת'] = df_only_neg['זמן_סיום_מהמערכת'].astype(str)
df_only_neg['time'] = df_only_neg['time'].astype(str)
df_only_neg['נ.צ כתובת'] = df_only_neg['נ.צ כתובת'].astype(str)
df_only_neg['סמל_יישוב'] = df_only_neg['סמל_יישוב'].astype(str)

df_only_neg['id'] = 'Date: ' + df_only_neg['זמן_סיום_מהמערכת'] + ' Time: ' + df_only_neg['time'] + ' Coordinates: ' + df_only_neg['נ.צ כתובת'] + ' City code: ' + df_only_neg['סמל_יישוב'] + ' Street: ' + df_only_neg['כתובת_תיאור_מיקום_נקודת_המדידה']

# Group by the ID and aggregate the counts
model_df = df_only_neg.groupby('id').agg({
    'זמן_סיום_מהמערכת': 'first',
    'time': 'first',
    'נ.צ כתובת': 'first',
    'סכום  אוכלוסייה בסוף השנה': 'first',
    'שטח': 'first',
    'דירוג חברתי-כלכלי ': 'first',
    'מדד סוציואקונומי': 'first',
    'כתובת_תיאור_מיקום_נקודת_המדידה': 'first',
    'מדרכה': 'first', 'אבנישפה': 'first', 'גדרות': 'first', 'צמחייה': 'first',
    'מצב הפח': 'first',
    'אריזות קרטון': 'first', 'כתמי מסטיק': 'first', 'צואת כלבים': 'first', 'פסולת אורגנית': 'first', 'פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות': 'first', 'זכוכית לא מכלי משקה או לא ניתן לזיהוי': 'first', 'אריזות של חטיפים': 'first', 'כוסות שתייה חמה חדפ': 'first', 'כוסות שתייה קרה חדפ': 'first', 'סכום חדפ': 'first', 'צלחות חדפ': 'first', 'אריזות מזון Take Away נייר': 'first', 'פקקים של מכלי משקה': 'first', 'מכלי משקה למיניהם': 'first', 'קופסאות סיגריות': 'first', 'מסכות כירורגיות': 'first', 'בדלי סיגריות': 'first',
    'כמה פחים יש בנקודת המדידה': 'first',
    'מפלס הפסולת בפח במספר': 'first',
    'חודש': 'first',
    'אזור תעשייה': 'first', 'חוף ים': 'first', 'מבנה ציבור': 'first', 'מרכז מסחרי': 'first', 'פנאי ונופש': 'first', 'רחוב מגורים': 'first', 'רחוב מסחרי': 'first',
    'ראשון': 'first', 'שני': 'first', 'שלישי': 'first', 'רביעי': 'first', 'חמישי': 'first', 'שישי': 'first', 'שבת': 'first',
    'פריט אחר': 'sum', 'כוס': 'sum', 'שקיות': 'sum', 'אריזות': 'sum', 'סיגריות': 'sum', 'בקבוקים': 'sum', 'צרכים בע"ח': 'sum', 'פלסטיק': 'sum', 'מסיכות': 'sum', 'פחיות': 'sum', 'נייר': 'sum', 'עטיפות': 'sum',
    'רכב': 'sum', 'הולך רגל': 'sum',
    'תיאור_חיובי_שלילי': 'count'  # Count the number of reports
}).rename(columns={'תיאור_חיובי_שלילי': 'num_of_reports'}).reset_index()

"""# **Changing Time and Month to dummy vars**"""

#Change time and month columns dummy vars
model_df = model_df.join(pd.get_dummies(model_df['חודש'], dtype=int, prefix='month'))
model_df = model_df.join(pd.get_dummies(model_df['time'], dtype=int,prefix='time'))

#Drop the original columns
model_df = model_df.drop(['חודש', 'time'], axis=1)

"""# **Min-Max Scaling**"""

#שטח
model_df['שטח'] = pd.to_numeric(model_df['שטח'], errors='coerce')
model_df['שטח'] = (model_df['שטח'] - model_df['שטח'].min()) / (model_df['שטח'].max() - model_df['שטח'].min())

#דירוג חברתי כלכלי
model_df['דירוג חברתי-כלכלי '] = pd.to_numeric(model_df['דירוג חברתי-כלכלי '], errors='coerce')
model_df['דירוג חברתי-כלכלי '] = (model_df['דירוג חברתי-כלכלי '] - model_df['דירוג חברתי-כלכלי '].min()) / (model_df['דירוג חברתי-כלכלי '].max() - model_df['דירוג חברתי-כלכלי '].min())

#סכום אוכלוסיה
model_df['סכום  אוכלוסייה בסוף השנה'] = pd.to_numeric(model_df['סכום  אוכלוסייה בסוף השנה'], errors='coerce')
model_df['סכום  אוכלוסייה בסוף השנה'] = (model_df['סכום  אוכלוסייה בסוף השנה'] - model_df['סכום  אוכלוסייה בסוף השנה'].min()) / (model_df['סכום  אוכלוסייה בסוף השנה'].max() - model_df['סכום  אוכלוסייה בסוף השנה'].min())

#אריזות קרטון
model_df['אריזות קרטון'] = pd.to_numeric(model_df['אריזות קרטון'], errors='coerce')
model_df['אריזות קרטון'] = (model_df['אריזות קרטון'] - model_df['אריזות קרטון'].min()) / (model_df['אריזות קרטון'].max() - model_df['אריזות קרטון'].min())

#כתמי מסטיק
model_df['כתמי מסטיק'] = pd.to_numeric(model_df['כתמי מסטיק'], errors='coerce')
model_df['כתמי מסטיק'] = (model_df['כתמי מסטיק'] - model_df['כתמי מסטיק'].min()) / (model_df['כתמי מסטיק'].max() - model_df['כתמי מסטיק'].min())

#צואת כלבים
model_df['צואת כלבים'] = pd.to_numeric(model_df['צואת כלבים'], errors='coerce')
model_df['צואת כלבים'] = (model_df['צואת כלבים'] - model_df['צואת כלבים'].min()) / (model_df['צואת כלבים'].max() - model_df['צואת כלבים'].min())

#פסולת אורגנית
model_df['פסולת אורגנית'] = pd.to_numeric(model_df['פסולת אורגנית'], errors='coerce')
model_df['פסולת אורגנית'] = (model_df['פסולת אורגנית'] - model_df['פסולת אורגנית'].min()) / (model_df['פסולת אורגנית'].max() - model_df['פסולת אורגנית'].min())

#פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות
model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'] = pd.to_numeric(model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'], errors='coerce')
model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'] = (model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'] - model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'].min()) / (model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'].max() - model_df['פלסטיק אחר שקיות פלסטיק ורכיבי פלסטיק שאינם מכלי משקה אריזות מזון ומשקאות'].min())

#זכוכית לא מכלי משקה או לא ניתן לזיהוי
model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'] = pd.to_numeric(model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'], errors='coerce')
model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'] = (model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'] - model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'].min()) / (model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'].max() - model_df['זכוכית לא מכלי משקה או לא ניתן לזיהוי'].min())

#אריזות של חטיפים
model_df['אריזות של חטיפים'] = pd.to_numeric(model_df['אריזות של חטיפים'], errors='coerce')
model_df['אריזות של חטיפים'] = (model_df['אריזות של חטיפים'] - model_df['אריזות של חטיפים'].min()) / (model_df['אריזות של חטיפים'].max() - model_df['אריזות של חטיפים'].min())

#כוסות שתייה חמה חדפ
model_df['כוסות שתייה חמה חדפ'] = pd.to_numeric(model_df['כוסות שתייה חמה חדפ'], errors='coerce')
model_df['כוסות שתייה חמה חדפ'] = (model_df['כוסות שתייה חמה חדפ'] - model_df['כוסות שתייה חמה חדפ'].min()) / (model_df['כוסות שתייה חמה חדפ'].max() - model_df['כוסות שתייה חמה חדפ'].min())

#כוסות שתייה קרה חדפ
model_df['כוסות שתייה קרה חדפ'] = pd.to_numeric(model_df['כוסות שתייה קרה חדפ'], errors='coerce')
model_df['כוסות שתייה קרה חדפ'] = (model_df['כוסות שתייה קרה חדפ'] - model_df['כוסות שתייה קרה חדפ'].min()) / (model_df['כוסות שתייה קרה חדפ'].max() - model_df['כוסות שתייה קרה חדפ'].min())

#סכום חדפ
model_df['סכום חדפ'] = pd.to_numeric(model_df['סכום חדפ'], errors='coerce')
model_df['סכום חדפ'] = (model_df['סכום חדפ'] - model_df['סכום חדפ'].min()) / (model_df['סכום חדפ'].max() - model_df['סכום חדפ'].min())

#צלחות חדפ
model_df['צלחות חדפ'] = pd.to_numeric(model_df['צלחות חדפ'], errors='coerce')
model_df['צלחות חדפ'] = (model_df['צלחות חדפ'] - model_df['צלחות חדפ'].min()) / (model_df['צלחות חדפ'].max() - model_df['צלחות חדפ'].min())

#אריזות מזון Take Away נייר
model_df['אריזות מזון Take Away נייר'] = pd.to_numeric(model_df['אריזות מזון Take Away נייר'], errors='coerce')
model_df['אריזות מזון Take Away נייר'] = (model_df['אריזות מזון Take Away נייר'] - model_df['אריזות מזון Take Away נייר'].min()) / (model_df['אריזות מזון Take Away נייר'].max() - model_df['אריזות מזון Take Away נייר'].min())

#פקקים של מכלי משקה
model_df['פקקים של מכלי משקה'] = pd.to_numeric(model_df['פקקים של מכלי משקה'], errors='coerce')
model_df['פקקים של מכלי משקה'] = (model_df['פקקים של מכלי משקה'] - model_df['פקקים של מכלי משקה'].min()) / (model_df['פקקים של מכלי משקה'].max() - model_df['פקקים של מכלי משקה'].min())

#מכלי משקה למיניהם
model_df['מכלי משקה למיניהם'] = pd.to_numeric(model_df['מכלי משקה למיניהם'], errors='coerce')
model_df['מכלי משקה למיניהם'] = (model_df['מכלי משקה למיניהם'] - model_df['מכלי משקה למיניהם'].min()) / (model_df['מכלי משקה למיניהם'].max() - model_df['מכלי משקה למיניהם'].min())

#קופסאות סיגריות
model_df['קופסאות סיגריות'] = pd.to_numeric(model_df['קופסאות סיגריות'], errors='coerce')
model_df['קופסאות סיגריות'] = (model_df['קופסאות סיגריות'] - model_df['קופסאות סיגריות'].min()) / (model_df['קופסאות סיגריות'].max() - model_df['קופסאות סיגריות'].min())

#מסכות כירורגיות
model_df['מסכות כירורגיות'] = pd.to_numeric(model_df['מסכות כירורגיות'], errors='coerce')
model_df['מסכות כירורגיות'] = (model_df['מסכות כירורגיות'] - model_df['מסכות כירורגיות'].min()) / (model_df['מסכות כירורגיות'].max() - model_df['מסכות כירורגיות'].min())

#בדלי סיגריות
model_df['בדלי סיגריות'] = pd.to_numeric(model_df['בדלי סיגריות'], errors='coerce')
model_df['בדלי סיגריות'] = (model_df['בדלי סיגריות'] - model_df['בדלי סיגריות'].min()) / (model_df['בדלי סיגריות'].max() - model_df['בדלי סיגריות'].min())

#כמה פחים יש בנקודת המדידה
model_df['כמה פחים יש בנקודת המדידה'] = pd.to_numeric(model_df['כמה פחים יש בנקודת המדידה'], errors='coerce')
model_df['כמה פחים יש בנקודת המדידה'] = (model_df['כמה פחים יש בנקודת המדידה'] - model_df['כמה פחים יש בנקודת המדידה'].min()) / (model_df['כמה פחים יש בנקודת המדידה'].max() - model_df['כמה פחים יש בנקודת המדידה'].min())

model_df.to_excel('model_df.xlsx', index=False)
print("model_df has been created.")

"""# **No nan rows / No nan columns DFs for the model**"""

# Split נ.צ כתובת into latitude and longitude
model_df[['latitude', 'longitude']] = model_df['נ.צ כתובת'].str.split(',', expand=True).astype(float)
model_df = model_df.drop(columns=['נ.צ כתובת'])

# Remove columns with any NaN values
model_df_noNanColumns = model_df.dropna(axis=1)

# Remove rows with any NaN values
model_df_noNanRows = model_df.dropna(axis=0)

model_df_noNanColumns.to_excel('model_df_noNanColumns.xlsx', index=False)
model_df_noNanRows.to_excel('model_df_noNanRows.xlsx', index=False)

"""# **No NAN Columns - LinearRegression / Random Forest / XGBoost models (num_of_reports)**"""

from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import datetime as dt
from sklearn.preprocessing import StandardScaler

#TARGET = num_of_reports

# Define features and target
X = model_df_noNanColumns.drop(columns=['num_of_reports', 'id', 'מדד סוציואקונומי', 'כתובת_תיאור_מיקום_נקודת_המדידה', 'זמן_סיום_מהמערכת', 'פריט אחר', 'כוס', 'שקיות', 'אריזות', 'סיגריות', 'בקבוקים', 'צרכים בע"ח', 'פלסטיק', 'מסיכות', 'פחיות', 'נייר', 'עטיפות', 'רכב', 'הולך רגל'])
y = model_df_noNanColumns['num_of_reports']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_train_pred_lin = lin_reg.predict(X_train)
y_test_pred_lin = lin_reg.predict(X_test)

# Calculate metrics for Linear Regression
r2_train_lin = r2_score(y_train, y_train_pred_lin)
r2_test_lin = r2_score(y_test, y_test_pred_lin)
rmse_train_lin = np.sqrt(mean_squared_error(y_train, y_train_pred_lin))
rmse_test_lin = np.sqrt(mean_squared_error(y_test, y_test_pred_lin))
std_dev_lin = np.std(y_test_pred_lin)

print(f'NO NAN COLUMNS\nTARGET = NUM_OF_REPORTS\n')
print(f'Linear Regression R² (Train): {r2_train_lin}')
print(f'Linear Regression R² (Test): {r2_test_lin}')
print(f'Linear Regression RMSE (Train): {rmse_train_lin}')
print(f'Linear Regression RMSE (Test): {rmse_test_lin}')
print(f'Linear Regression Std Dev (Test Predictions): {std_dev_lin}\n')

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_train_pred_rf = rf_reg.predict(X_train)
y_test_pred_rf = rf_reg.predict(X_test)

# Calculate metrics for Random Forest Regression
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_train_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
std_dev_rf = np.std(y_test_pred_rf)

print(f'Random Forest Regression R² (Train): {r2_train_rf}')
print(f'Random Forest Regression R² (Test): {r2_test_rf}')
print(f'Random Forest Regression RMSE (Train): {rmse_train_rf}')
print(f'Random Forest Regression RMSE (Test): {rmse_test_rf}')
print(f'Random Forest Regression Std Dev (Test Predictions): {std_dev_rf}\n')

# XGBoost Regression
xgb_reg = XGBRegressor(n_estimators=100, random_state=42)
xgb_reg.fit(X_train, y_train)
y_train_pred_xgb = xgb_reg.predict(X_train)
y_test_pred_xgb = xgb_reg.predict(X_test)

# Calculate metrics for XGBoost Regression
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)
r2_test_xgb = r2_score(y_test, y_test_pred_xgb)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
std_dev_xgb = np.std(y_test_pred_xgb)

print(f'XGBoost Regression R² (Train): {r2_train_xgb}')
print(f'XGBoost Regression R² (Test): {r2_test_xgb}')
print(f'XGBoost Regression RMSE (Train): {rmse_train_xgb}')
print(f'XGBoost Regression RMSE (Test): {rmse_test_xgb}')
print(f'XGBoost Regression Std Dev (Test Predictions): {std_dev_xgb}')

X.to_excel('model_df_noNanColumns_afterDrop.xlsx', index=False)

feature_importances = rf_reg.feature_importances_
feature_names = X.columns

import matplotlib.pyplot as plt
import numpy as np

# Sort the features by importance
indices = np.argsort(feature_importances)

# Create the plot
plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(range(len(indices)), feature_importances[indices], align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""# **No NAN Rows - LinearRegression / Random Forest / XGBoost models (num_of_reports)**"""

#TARGET = num_of_reports

# Define features and target
X = model_df_noNanRows.drop(columns=['num_of_reports', 'id', 'כתובת_תיאור_מיקום_נקודת_המדידה', 'זמן_סיום_מהמערכת', 'מדד סוציואקונומי', 'פריט אחר', 'כוס', 'שקיות', 'אריזות', 'סיגריות', 'בקבוקים', 'צרכים בע"ח', 'פלסטיק', 'מסיכות', 'פחיות', 'נייר', 'עטיפות', 'רכב', 'הולך רגל'])
y = model_df_noNanRows['num_of_reports']

# Convert date to numeric
#X['זמן_סיום_מהמערכת'] = pd.to_datetime(X['זמן_סיום_מהמערכת'], format = '%d-%m-%Y')
#X['זמן_סיום_מהמערכת'] = X['זמן_סיום_מהמערכת'].map(dt.datetime.toordinal)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_train_pred_lin = lin_reg.predict(X_train)
y_test_pred_lin = lin_reg.predict(X_test)

# Calculate metrics for Linear Regression
r2_train_lin = r2_score(y_train, y_train_pred_lin)
r2_test_lin = r2_score(y_test, y_test_pred_lin)
rmse_train_lin = np.sqrt(mean_squared_error(y_train, y_train_pred_lin))
rmse_test_lin = np.sqrt(mean_squared_error(y_test, y_test_pred_lin))
std_dev_lin = np.std(y_test_pred_lin)

print(f'NO NAN ROWS\nTARGET = NUM_OF_REPORTS\n')
print(f'Linear Regression R² (Train): {r2_train_lin}')
print(f'Linear Regression R² (Test): {r2_test_lin}')
print(f'Linear Regression RMSE (Train): {rmse_train_lin}')
print(f'Linear Regression RMSE (Test): {rmse_test_lin}')
print(f'Linear Regression Std Dev (Test Predictions): {std_dev_lin}\n')

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_train_pred_rf = rf_reg.predict(X_train)
y_test_pred_rf = rf_reg.predict(X_test)

# Calculate metrics for Random Forest Regression
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_train_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
std_dev_rf = np.std(y_test_pred_rf)

print(f'Random Forest Regression R² (Train): {r2_train_rf}')
print(f'Random Forest Regression R² (Test): {r2_test_rf}')
print(f'Random Forest Regression RMSE (Train): {rmse_train_rf}')
print(f'Random Forest Regression RMSE (Test): {rmse_test_rf}')
print(f'Random Forest Regression Std Dev (Test Predictions): {std_dev_rf}\n')

# XGBoost Regression
xgb_reg = XGBRegressor(n_estimators=100, random_state=42)
xgb_reg.fit(X_train, y_train)
y_train_pred_xgb = xgb_reg.predict(X_train)
y_test_pred_xgb = xgb_reg.predict(X_test)

# Calculate metrics for XGBoost Regression
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)
r2_test_xgb = r2_score(y_test, y_test_pred_xgb)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
std_dev_xgb = np.std(y_test_pred_xgb)

print(f'XGBoost Regression R² (Train): {r2_train_xgb}')
print(f'XGBoost Regression R² (Test): {r2_test_xgb}')
print(f'XGBoost Regression RMSE (Train): {rmse_train_xgb}')
print(f'XGBoost Regression RMSE (Test): {rmse_test_xgb}')
print(f'XGBoost Regression Std Dev (Test Predictions): {std_dev_xgb}')

X.to_excel('model_df_noNanRows_afterDrop.xlsx', index=False)

"""# **No NAN Columns - LinearRegression / Random Forest / XGBoost models (הולך רגל)**"""

#TARGET = הולך רגל

# Define features and target
X = model_df_noNanColumns.drop(columns=['הולך רגל', 'id', 'כתובת_תיאור_מיקום_נקודת_המדידה'])
y = model_df_noNanColumns['הולך רגל']

# Convert date to numeric
X['זמן_סיום_מהמערכת'] = pd.to_datetime(X['זמן_סיום_מהמערכת'], format = '%d-%m-%Y')
X['זמן_סיום_מהמערכת'] = X['זמן_סיום_מהמערכת'].map(dt.datetime.toordinal)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_train_pred_lin = lin_reg.predict(X_train)
y_test_pred_lin = lin_reg.predict(X_test)

# Calculate metrics for Linear Regression
r2_train_lin = r2_score(y_train, y_train_pred_lin)
r2_test_lin = r2_score(y_test, y_test_pred_lin)
rmse_train_lin = np.sqrt(mean_squared_error(y_train, y_train_pred_lin))
rmse_test_lin = np.sqrt(mean_squared_error(y_test, y_test_pred_lin))
std_dev_lin = np.std(y_test_pred_lin)

print(f'NO NAN COLUMNS\nTARGET = הולך רגל\n')
print(f'Linear Regression R² (Train): {r2_train_lin}')
print(f'Linear Regression R² (Test): {r2_test_lin}')
print(f'Linear Regression RMSE (Train): {rmse_train_lin}')
print(f'Linear Regression RMSE (Test): {rmse_test_lin}')
print(f'Linear Regression Std Dev (Test Predictions): {std_dev_lin}\n')

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_train_pred_rf = rf_reg.predict(X_train)
y_test_pred_rf = rf_reg.predict(X_test)

# Calculate metrics for Random Forest Regression
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_train_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
std_dev_rf = np.std(y_test_pred_rf)

print(f'Random Forest Regression R² (Train): {r2_train_rf}')
print(f'Random Forest Regression R² (Test): {r2_test_rf}')
print(f'Random Forest Regression RMSE (Train): {rmse_train_rf}')
print(f'Random Forest Regression RMSE (Test): {rmse_test_rf}')
print(f'Random Forest Regression Std Dev (Test Predictions): {std_dev_rf}\n')

# XGBoost Regression
xgb_reg = XGBRegressor(n_estimators=100, random_state=42)
xgb_reg.fit(X_train, y_train)
y_train_pred_xgb = xgb_reg.predict(X_train)
y_test_pred_xgb = xgb_reg.predict(X_test)

# Calculate metrics for XGBoost Regression
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)
r2_test_xgb = r2_score(y_test, y_test_pred_xgb)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
std_dev_xgb = np.std(y_test_pred_xgb)

print(f'XGBoost Regression R² (Train): {r2_train_xgb}')
print(f'XGBoost Regression R² (Test): {r2_test_xgb}')
print(f'XGBoost Regression RMSE (Train): {rmse_train_xgb}')
print(f'XGBoost Regression RMSE (Test): {rmse_test_xgb}')
print(f'XGBoost Regression Std Dev (Test Predictions): {std_dev_xgb}')

"""# **No NAN Rows - LinearRegression / Random Forest / XGBoost models (הולך רגל)**"""

#TARGET = הולך רגל

# Define features and target
X = model_df_noNanRows.drop(columns=['הולך רגל', 'id', 'כתובת_תיאור_מיקום_נקודת_המדידה'])
y = model_df_noNanRows['הולך רגל']

# Convert date to numeric
X['זמן_סיום_מהמערכת'] = pd.to_datetime(X['זמן_סיום_מהמערכת'], format = '%d-%m-%Y')
X['זמן_סיום_מהמערכת'] = X['זמן_סיום_מהמערכת'].map(dt.datetime.toordinal)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_train_pred_lin = lin_reg.predict(X_train)
y_test_pred_lin = lin_reg.predict(X_test)

# Calculate metrics for Linear Regression
r2_train_lin = r2_score(y_train, y_train_pred_lin)
r2_test_lin = r2_score(y_test, y_test_pred_lin)
rmse_train_lin = np.sqrt(mean_squared_error(y_train, y_train_pred_lin))
rmse_test_lin = np.sqrt(mean_squared_error(y_test, y_test_pred_lin))
std_dev_lin = np.std(y_test_pred_lin)

print(f'NO NAN ROWS\nTARGET = הולך רגל\n')
print(f'Linear Regression R² (Train): {r2_train_lin}')
print(f'Linear Regression R² (Test): {r2_test_lin}')
print(f'Linear Regression RMSE (Train): {rmse_train_lin}')
print(f'Linear Regression RMSE (Test): {rmse_test_lin}')
print(f'Linear Regression Std Dev (Test Predictions): {std_dev_lin}\n')

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_train_pred_rf = rf_reg.predict(X_train)
y_test_pred_rf = rf_reg.predict(X_test)

# Calculate metrics for Random Forest Regression
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_train_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
std_dev_rf = np.std(y_test_pred_rf)

print(f'Random Forest Regression R² (Train): {r2_train_rf}')
print(f'Random Forest Regression R² (Test): {r2_test_rf}')
print(f'Random Forest Regression RMSE (Train): {rmse_train_rf}')
print(f'Random Forest Regression RMSE (Test): {rmse_test_rf}')
print(f'Random Forest Regression Std Dev (Test Predictions): {std_dev_rf}\n')

# XGBoost Regression
xgb_reg = XGBRegressor(n_estimators=100, random_state=42)
xgb_reg.fit(X_train, y_train)
y_train_pred_xgb = xgb_reg.predict(X_train)
y_test_pred_xgb = xgb_reg.predict(X_test)

# Calculate metrics for XGBoost Regression
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)
r2_test_xgb = r2_score(y_test, y_test_pred_xgb)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
std_dev_xgb = np.std(y_test_pred_xgb)

print(f'XGBoost Regression R² (Train): {r2_train_xgb}')
print(f'XGBoost Regression R² (Test): {r2_test_xgb}')
print(f'XGBoost Regression RMSE (Train): {rmse_train_xgb}')
print(f'XGBoost Regression RMSE (Test): {rmse_test_xgb}')
print(f'XGBoost Regression Std Dev (Test Predictions): {std_dev_xgb}')

"""# **Turning the data for Classification**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Assuming `model_df_noNanColumns` is your initial DataFrame
model_df_noNanColumns_Classes = model_df_noNanColumns

# Calculate the 33rd and 66th percentiles
lower_threshold = model_df_noNanColumns_Classes['num_of_reports'].quantile(0.3)
upper_threshold = model_df_noNanColumns_Classes['num_of_reports'].quantile(0.66)

# Create the classes
model_df_noNanColumns_Classes['target'] = pd.cut(model_df_noNanColumns_Classes['num_of_reports'],
                            bins=[-np.inf, lower_threshold, upper_threshold, np.inf],
                            labels=['Low', 'Medium', 'High'])

# Summarize class distribution
class_summary = model_df_noNanColumns_Classes['target'].value_counts()
class_summary_pct = model_df_noNanColumns_Classes['target'].value_counts(normalize=True)

summary = pd.concat([class_summary, class_summary_pct], axis=1, keys=['Count', 'Percentage'])
print(summary)

model_df_noNanColumns_Classes.to_excel('model_df_noNanColumns_Classes.xlsx', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score

# Load the dataset
file_path = 'model_df_noNanColumns_Classes.xlsx'
df = pd.read_excel(file_path)

# Encode the target variable
label_encoder = LabelEncoder()
df['target'] = label_encoder.fit_transform(df['target'])

# Drop the specified columns
X = df.drop(columns=[
    'target', 'num_of_reports', 'id', 'מדד סוציואקונומי', 'כתובת_תיאור_מיקום_נקודת_המדידה',
    'זמן_סיום_מהמערכת', 'פריט אחר', 'כוס', 'שקיות', 'אריזות', 'סיגריות', 'בקבוקים',
    'צרכים בע"ח', 'פלסטיק', 'מסיכות', 'פחיות', 'נייר', 'עטיפות', 'רכב', 'הולך רגל',
    ])
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)

# Initialize the model with the best parameters
gb = GradientBoostingClassifier(random_state=42)

# Train the model on the entire training set
gb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = gb.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)

print(accuracy)
print(report)

# Running the training and evaluation process multiple times with correct initialization

results = []

for i in range(5):  # Run the process 5 times
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i*42)

    # Initialize the model with the best parameters
    gb = GradientBoostingClassifier(random_state=42)

    # Train the model on the entire training set
    gb.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = gb.predict(X_test)

    # Evaluate the model
    print(f"Iteration: {i+1}")
    accuracy = accuracy_score(y_test, y_pred)
    print(accuracy)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)
    print(report)

    results.append({
        "Iteration": i + 1,
        "Accuracy": accuracy,
        "Classification Report": report
    })

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Plot the confusion matrix
ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test, display_labels=label_encoder.classes_, cmap=plt.cm.Blues)
plt.title("Confusion Matrix for the Best Gradient Boosting Model")
plt.show()